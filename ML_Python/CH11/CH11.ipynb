{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573014e8",
   "metadata": {},
   "source": [
    "# Chapter 11 - Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8550a26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5911f6d9",
   "metadata": {},
   "source": [
    "## First off, when and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35761f97",
   "metadata": {},
   "source": [
    "- Complex problems, like detecting hundreds of types of objects in a low-res image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b23dbc",
   "metadata": {},
   "source": [
    "## Problems to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eccbcb",
   "metadata": {},
   "source": [
    "- **Vanishing/exploding gradients**: Gradients grow exponentially larger/smaller as you backpropagate through the layers of neurons, meaning that lower layers are much harder to train.\n",
    "\n",
    "- Lack of data/labeled data\n",
    "\n",
    "- Slow training\n",
    "\n",
    "- Risk of overfitting w/ a large # of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619466d",
   "metadata": {},
   "source": [
    "$$\\\\\\\\$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c83783",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91f921",
   "metadata": {},
   "source": [
    "Because the gradient of sigmoid and tanh functions approaches 0 at large input values, this effect is only propagated over many layers. A glorot initialization is often used to combat this problem:\n",
    "\n",
    "**Fan-in** - Number of inputs\n",
    "\n",
    "**Fan-out** - Number of neurons in a layer\n",
    "\n",
    "**Glorot/Xavier initialization**: \n",
    "- Normal distribution w/ mean 0 and Variance $\\sigma^2 = \\frac{1}{\\text{fan}_{avg}}$\n",
    "- Uniform distribution between $-r$ and $r$ w/ $r=\\sqrt{\\frac{3}{\\text{fan}_{avg}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c9dba",
   "metadata": {},
   "source": [
    "### Activation functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0c829",
   "metadata": {},
   "source": [
    "Certain activation functions can cause the 'death' of neurons. For example, RELU functions have a 0 gradient when x<0, which can cause the neuron to die and stop changing. Because of this, **Leaky ReLU** is used. This adds a small downward slope for values $x<0$.\n",
    "\n",
    "Also, the Exponential Linear Unit (ELU)\n",
    "$\\begin{cases} \n",
    "      \\alpha (\\exp(x)-1) & x<0\\\\\n",
    "      x & x\\geq 0\n",
    "   \\end{cases}\n",
    "$\n",
    "\n",
    "where $\\alpha$ is a hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017d3069",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4745b9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
